[[package]]
name = "py4j"
version = "0.10.9.7"
description = "Enables Python programs to dynamically access arbitrary Java objects"
category = "main"
optional = false
python-versions = "*"

[[package]]
name = "pyspark"
version = "3.5.1"
description = "Apache Spark Python API"
category = "main"
optional = false
python-versions = ">=3.8"

[package.dependencies]
py4j = "0.10.9.7"

[package.extras]
connect = ["googleapis-common-protos (>=1.56.4)", "grpcio-status (>=1.56.0)", "grpcio (>=1.56.0)", "numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=4.0.0)"]
ml = ["numpy (>=1.15)"]
mllib = ["numpy (>=1.15)"]
pandas_on_spark = ["numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=4.0.0)"]
sql = ["numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=4.0.0)"]

[metadata]
lock-version = "1.1"
python-versions = "^3.9"
content-hash = "d1b28ec5dafbfd460d392c606543d5b8f2aec57ad9f1e7a6b8f1b905dd624e5e"

[metadata.files]
py4j = []
pyspark = []
